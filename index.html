<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, .initial-scale=1">
  <title>Nerfies: Deformable Neural Radiance Fields</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script type="text/javascript" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>
-->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Nerfies: Deformable Neural Radiance Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kodaikawamura.github.io/kodaikawamura/">Kodai Kawamura*</a><sup>*1,2</sup>,</span>
            <!--
            <span class="author-block">
              <a href="https://utkarshsinha.com">Yuta Goto*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Rintaro Yanagi</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Hirokatsu Kataoka</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Go Irie</a><sup>1</sup>,
            </span>-->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tokyo University of Science,</span>
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
            <span class="author-block"><sup>3</sup>National Institute of Advanced Industrial Science and Technology (AIST),</span>
            <span class="author-block"><sup>4</sup>Visual Geometry Group, University of Oxford</span>
            <span class="author-block">* equal contribution.</span>
          </div>

          <div class="is-size-4 publication-authors">
                    <b>Preprint 2025</b>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
                  
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. 
            However, they often retain irrelevant information beyond the requirements of specific target downstream tasks, raising concerns about computational efficiency and potential information leakage. 
            This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance.
            Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. 
            However, merely forgetting object classes is often insufficient in practical applications. 
            For instance, an autonomous driving system should accurately recognize real cars, while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. 
            In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real).           
            ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. 
            To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information.
             Extensive experiments on three multi-domain benchmark datasets demonstrate that our approach significantly outperforms strong baselines built upon state-of-the-art VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs.
          </p>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Approximate Domain Unlearning</h2>
        <div class="content has-text-centered">
            <img src="./static/images/fig1.pdf" alt="Illustration of Approximate Domain Unlearning (ADU)" width="80%">
        </div>
        <div class="content has-text-justified">
            <p>Approximate Domain Unlearning (ADU) is a novel approximate unlearning problem introduced in this paper. </p>
            <p>Unlike existing approximate class unlearning tasks, ADU requires retraining a pre-trained Vision-Language Model (VLM) so that it cannot recognize images from specified domains (painting, clipart, sketch in the figure) while preserving its ability to recognize images from other domains (real in the figure). </p>
        </div>
        <h3 class="title is-4">Problem Setting</h3>
        <div class="content has-text-justified">
          <p>
            Given a set of training data
            \( \{(\mathbf{x}, y, d)\} \),
            where \( \mathbf{x} \in \mathcal{X} \) represents an input image,
            \( y \in \mathcal{C} \) is the class label, and
            \( d \in \mathcal{D} \) is the domain label, with
            \( \mathcal{C} \) and \( \mathcal{D} \) denoting the sets of all classes
            and domains, respectively.
          </p>

          <p class="math-block">
            We define
            \( \mathcal{D}_{\text{memorize}} \subset \mathcal{D} \)
            as the set of domains to be preserved and
            \( \mathcal{D}_{\text{forget}} = \mathcal{D} \setminus \mathcal{D}_{\text{memorize}} \)
            as the set of domains to be forgotten.
          </p>

          <p class="math-block">
            Our goal is to retrain a pre-trained vision-language model \( f \) to
            maintain the classification accuracy for
            \( \{(\mathbf{x}, y, d) \mid d \in \mathcal{D}_{\text{memorize}}\} \),
            while reducing it for
            \( \{(\mathbf{x}, y, d) \mid d \in \mathcal{D}_{\text{forget}}\} \).
          </p>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <h3 class="title is-4">Applying Common Approximate Class Unlearning Approach to ADU</h3>
        <div class="content has-text-justified">
          <p>
            The common approach to the conventional approximate class unlearning is to use two different loss functions; one for retaining the classification accuracy for the classes to be retained and the other for reducing the accuracy for those to be forgotten.
          </p>
          <p>
            Given this idea, a straightforward approach to ADU would be to adapt these two loss functions, that is, minimizing 
            \( \mathcal{L}_{\mathrm{memorize}} \) for 
            \( \{(x, y, d) \mid d \in \mathcal{D}_{\text{memorize}}\} \) 
            and 
            \( \mathcal{L}_{\mathrm{forget}} \) for 
            \( \{(x, y, d) \mid d \in \mathcal{D}_{\text{forget}}\} \).
          </p>
          <p>
            However, as we will show later in our experiments, this straightforward approach alone is insufficient to achieve satisfactory ADU performance.
          </p>
          <p>
            This is primarily due to the strong domain generalization capability of pre-trained VLMs.
          </p>
          <p>
            As evidenced by their robustness to domain shifts, the latent space of VLMs highly aligns data distributions across different domains, meaning that covariate shifts are minimal.  
          </p>
          <p>
            Consequently, the feature distributions across different domains are highly entangled, making it difficult to effectively control memorization and forgetting on a per-domain basis.
          </p>
        </div>
        <h3 class="title is-4">Reward-Adaptive Time-Rescaled Langevin SDE</h3>
        <div class="content has-text-centered">
            <img src="./static/images/algorithm_v2.png" alt="Reward-Adaptive Time-Rescaled Langevin SDE" width="50%">
        </div>
        <div class="content has-text-justified">
            <p>
                We introduce <b>Reward-Guided Langevin Dynamics</b>, to efficiently sample a latent representation \(\mathbf{x}\) from the optimal reward-aligned distribution. Unlike traditional gradient ascent, which may get stuck in local optima, this approach incorporates stochasticity, leading to the following simple discretized update rule: <br>
                \[ \mathbf{x}_{i+1} = \sqrt{1-\gamma}\, \mathbf{x}_i + \gamma\eta \nabla \hat{\mathcal{R}}(\mathbf{x}_i) + \sqrt{\gamma} \epsilon_{i}. \]
                Note that for implementation, this requires only <b>a single line of code</b>, to add Gaussian noise \(\epsilon_i \sim \mathcal{N}(0, \mathbf{I})\) to the latent representation. <br><br>

                To further enhance convergence speed and performance, we introduce <b>Reward-Adaptive Time Rescaling</b>, modifying the Langevin process with a monitor function \(\mathcal{G}(\hat{\mathcal{R}}(\mathbf{x}))\) that dynamically adjusts the step size based on the reward: <br>
                \[ \mathbf{x}_{i+1} =\sqrt{1-\gamma(\mathbf{x}_i)}\mathbf{x}_i + \gamma(\mathbf{x}_i)\eta\,\nabla \hat{\mathcal{R}}(\mathbf{x}_i) + \frac{1}{2}\gamma(\mathbf{x}_i)\nabla\log \mathcal{G}(\hat{\mathcal{R}}(\mathbf{x}_i)) + \sqrt{\gamma(\mathbf{x}_i)}\,\epsilon_i. \]

            </p>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>
        <h3 class="title is-4">Experimental Setup</h3>
        <div class="content has-text-justified">
            <p>
                We introduce ORIBENCH benchmark based on the MS-COCO dataset, that consist of diverse text prompts, image and ground truth orientations.<br>
                <b>1. ORIBENCH-Single</b>: Single Object Scenario, total 1000 prompts, various azimuths.<br>
                <b>2. ORIBENCH-Multi</b>: Multi Object Scenario, total 371 prompts, various azimuths.<br>
                For more evaluation (General Orientation, primitive views), please refer to the Appendix of the paper.
            </p>
        </div>
        <h3 class="title is-4">Quantitative Results</h3>
        <div class="content has-text-centered">
            <img src="./static/images/quantitative_v2.png" alt="Quantitative Results" width="80%">
        </div>
        <div class="content has-text-justified">
            <p>
                We measure <b>orientation grounding accuracy</b> using two metrics: <br>
                <b>1) Absolute Error,</b> the absolute error on azimuth angles between the predicted and grounding object orientations. <br>
                <b>2) Acc.@22.5°,</b> the angular accuracy within a tolerance of ±22.5°. <br>
                For evaluation, we use OrientAnything to predict the 3D orientation from the generated images. <br>
                In the case of <b>text-image alignment,</b> we use three metrics: <b>CLIP Score</b>, <b>VQA-Score</b>, and <b>PickScore</b>.
            </p>
        </div>
        <h3 class="title is-4">ORIBENCH-Single</h3>
        <div class="content has-text-centered">
            <img src="./static/images/quali/quali_single_v2.png" alt="MS-COCO-Single" width="100%">
        </div>
        <div class="content has-text-justified">
            <p>
                C3DW (Cheng et al.) is trained on synthetic data to learn orientation-to-image generation. Thus, it has limited generalizability to real-world images and the output images lack realism.
                Zero-1-to-3 (Liu et al.) is also trained on single-object images but without backgrounds, requiring additional background image composition that may introduce unnatrual artifacts.
                The existing methods (DPS, MPGD, FreeDoM, ReNO) on guided generation methods also achieve suboptimal results compared to ORIGEN.
            </p>
        </div>
        <h3 class="title is-4">ORIBENCH-Multi</h3>
        <div class="content has-text-centered">
            <div class="image-container">
                <img src="./static/images/quali/quali_multi_v2.png" alt="MS-COCO-NView" width="100%"> 
            </div>
        </div>
        <div class="content has-text-justified">
            <p>   
                ORIGEN outperforms all baseline models in orientation alignment. While FLUX-Schnell achieves the highest alignment among vanilla T2I models, ORIGEN surpasses it by over 2.5 times in the 3-view setting (82.4% vs. 31.2%) and over 2 times in the 4-view setting (86.6% vs. 42.4%). 
                This highlights the limitations of vanilla T2I models, which struggle with precise orientation control due to the ambiguity in textual descriptions. 
                Unlike these models, ORIGEN consistently generates images that accurately align with the specified orientations.
            </p>
        </div>
        <h3 class="title is-4">User Study</h3>
        <div class="content has-text-centered">
            <img src="./static/images/user_study.jpg" alt="User study results" width="50%">
        </div>
        <div class="content has-text-justified">
            <p>
                The input prompt and images generated by three models (Zero-1-to-3, C3DW, and ORIGEN) were provided, and participants were asked to select the image that best reflected both the input prompt and the grounding orientation. 
                As a result, ORIGEN was preferred by 58.18% of the participants, outperforming the baseline models.
            </p>
        </div>
    </div>
</section>



<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and <a href="https://joonghyuk.com/instantdrag-web/">InstantDrag</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and <a href="https://joonghyuk.com/instantdrag-web/">InstantDrag</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>


</body>
</html>

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, .initial-scale=1">
  <title>Nerfies: Deformable Neural Radiance Fields</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
        src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script type="text/javascript" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>
-->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Approximate Domain Unlearning <br> for Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kodaikawamura.github.io/kodaikawamura/">Kodai Kawamura*</a><sup>*1,2</sup>,</span>
            <span class="author-block">
              <a href="http://www.linkedin.com/in/%E5%84%AA%E5%A4%AA-%E5%BE%8C%E8%97%A4-54b625361">Yuta Goto*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4dA9-SoAAAAJ">Rintaro Yanagi</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://hirokatsukataoka.net/">Hirokatsu Kataoka</a><sup>3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=2bCSG1AAAAAJ">Go Irie</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tokyo University of Science,</span>
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
            <span class="author-block"><sup>3</sup>National Institute of Advanced Industrial Science and Technology (AIST),</span>
            <span class="author-block"><sup>4</sup>Visual Geometry Group, University of Oxford</span>
            <span class="author-block">* equal contribution.</span>
          </div>

          <div class="is-size-4 publication-authors">
                    <b>Preprint 2025</b>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
                  
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="content has-text-centered">
    <img src="./static/images/overview.png" alt="Illustration of Approximate Domain Unlearning (ADU)" width="50%">
</div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. 
            However, they often retain irrelevant information beyond the requirements of specific target downstream tasks, raising concerns about computational efficiency and potential information leakage. 
            This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance.
            Existing approaches to approximate unlearning have primarily focused on class unlearning, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. 
            However, merely forgetting object classes is often insufficient in practical applications. 
            For instance, an autonomous driving system should accurately recognize real cars, while avoiding misrecognition of illustrated cars depicted in roadside advertisements as real cars, which could be hazardous. 
            In this paper, we introduce Approximate Domain Unlearning (ADU), a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., illustration) while preserving accuracy for other domains (e.g., real).           
            ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. 
            To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information.
             Extensive experiments on three multi-domain benchmark datasets demonstrate that our approach significantly outperforms strong baselines built upon state-of-the-art VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs.
          </p>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Approximate Domain Unlearning (ADU)</h2>
        <div class="content has-text-centered">
            <img src="./static/images/fig1.png" alt="Illustration of Approximate Domain Unlearning (ADU)" width="40%">
        </div>
        <div class="content has-text-justified">
            <p>Approximate Domain Unlearning (ADU) is a novel approximate unlearning problem introduced in this paper. Unlike existing approximate class unlearning tasks, ADU requires retraining a pre-trained Vision-Language Model (VLM) so that it cannot recognize images from specified domains (painting, clipart, sketch in the figure) while preserving its ability to recognize images from other domains (real in the figure). </p>
        </div>
        <h3 class="title is-4">Problem Setting</h3>
        <div class="content has-text-justified">
          <p>
            Given a set of training data
            \( \{(\mathbf{x}, y, d)\} \),
            where \( \mathbf{x} \in \mathcal{X} \) represents an input image,
            \( y \in \mathcal{C} \) is the class label, and
            \( d \in \mathcal{D} \) is the domain label, with
            \( \mathcal{C} \) and \( \mathcal{D} \) denoting the sets of all classes
            and domains, respectively.
            We define
            \( \mathcal{D}_{\text{memorize}} \subset \mathcal{D} \)
            as the set of domains to be preserved and
            \( \mathcal{D}_{\text{forget}} = \mathcal{D} \setminus \mathcal{D}_{\text{memorize}} \)
            as the set of domains to be forgotten.
            Our goal is to retrain a pre-trained vision-language model \( f \) to
            maintain the classification accuracy for
            \( \{(\mathbf{x}, y, d) \mid d \in \mathcal{D}_{\text{memorize}}\} \),
            while reducing it for
            \( \{(\mathbf{x}, y, d) \mid d \in \mathcal{D}_{\text{forget}}\} \).
          </p>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <h3 class="title is-4">Applying Common Approximate Class Unlearning Approach to ADU</h3>
        <div class="content has-text-justified">
          <p>
            The common approach to the conventional approximate class unlearning is to use two different loss functions; one for retaining the classification accuracy for the classes to be retained and the other for reducing the accuracy for those to be forgotten.
          </p>
          <p>
            Given this idea, a straightforward approach to ADU would be to adapt these two loss functions, that is, minimizing 
            \( \mathcal{L}_{\mathrm{memorize}} \) for 
            \( \{(x, y, d) \mid d \in \mathcal{D}_{\text{memorize}}\} \) 
            and 
            \( \mathcal{L}_{\mathrm{forget}} \) for 
            \( \{(x, y, d) \mid d \in \mathcal{D}_{\text{forget}}\} \).
          </p>
          <p>
            However, as we will show later in our experiments, this straightforward approach alone is insufficient to achieve satisfactory ADU performance.
          </p>
          <p>
            This is primarily due to the strong domain generalization capability of pre-trained VLMs.
          </p>
          <p>
            As evidenced by their robustness to domain shifts, the latent space of VLMs highly aligns data distributions across different domains, meaning that covariate shifts are minimal.  
          </p>
          <p>
            Consequently, the feature distributions across different domains are highly entangled, making it difficult to effectively control memorization and forgetting on a per-domain basis.
          </p>
        </div>
        <h3 class="title is-4">Domain Disentangling Loss (DDL)</h3>
        <div class="content has-text-justified">
          <p>
            To address this issue, we propose Domain Disentangling Loss (DDL), which aims to explicitly disentangle the feature distributions among different domains in the latent feature space.
          </p>

          <p>
            The core idea is that if the feature distributions of individual domains are well-separated, the domain label \( d \) of a given sample \( \mathbf{x} \) can be accurately predicted, and vice versa.
          </p>

          <p>
            Based on this insight, DDL encourages the domain label of a sample to be predictable from its latent feature through an auxiliary domain classifier.
          </p>

          <p>
            More specifically, we introduce a standard cross-entropy loss that requires the model to correctly predict the domain labels of the samples:
          </p>

          <p>
          \[
          \mathcal{L}_{\mathrm{CE}}(\mathcal{B})=-\frac{1}{|\mathcal{B}|}\sum_{i=1}^{|\mathcal{B}|} \sum_{j=1}^{|\mathcal{D}|} d_{ij}\log p^d_{ij},
          \]
          </p>

          <p>
            where \( \mathbf{p}_i^d=(p^d_{i1}, p^d_{i2}, \dots, p^d_{i|\mathcal{D}|}) \) represents the confidence scores of a sample \( \mathbf{x}_i \) output by the domain classifier (a fully connected layer), and \( \mathbf{d}_i=(d_{i1}, d_{i2}, \dots, d_{i|\mathcal{D}|}) \) is the one-hot encoding of the domain label \( d_i \).
          </p>

          <p>
            To further enhance domain separability, we additionally incorporate the maximum mean discrepancy (MMD) into DDL as an auxiliary loss term. MMD estimates the pairwise distance between domain distributions in a reproducing kernel Hilbert space (RKHS) as:
          </p>

          <p>
          \[
          \text{MMD}^2(\mathcal{B}) = 
          \frac{2}{|\mathcal{D}|(|\mathcal{D}|-1)} \sum_{1 \leq d < d' \leq |\mathcal{D}|} 
          \left\| 
          \frac{1}{|\mathcal{B}_d|} \sum_{\mathbf{x}_i \in \mathcal{B}_d} \phi(\mathbf{x}_i) - 
          \frac{1}{|\mathcal{B}_{d'}|} \sum_{\mathbf{x}_j \in \mathcal{B}_{d'}} \phi(\mathbf{x}_j) 
          \right\|_{\mathcal{H}}^2,
          \]
          </p>

          <p>
            where \( \phi \) denotes a kernel-induced feature mapping, \( \mathcal{B}_d \) is a subset of mini-batch \( \mathcal{B} \) within the domain \( d \in \mathcal{D} \).
            Intuitively, maximizing MMD increases the inter-domain divergence in the latent space.
          </p>

          <p>
            Given the above formulations, our final DDL loss is defined as:
          </p>

          <p>
          \[
          \mathcal{L}_{\mathrm{domain}}(\mathcal{B}) = \gamma \mathcal{L}_{\mathrm{CE}}(\mathcal{B}) - \lambda \text{MMD}^2(\mathcal{B}),
          \]
          </p>

          <p>
            where \( \gamma \) and \( \lambda \) are balancing hyperparameters.
          </p>

          <p>
            Combining with the standard loss functions \( \mathcal{L}_{\mathrm{memorize}} \) and \( \mathcal{L}_{\mathrm{forget}} \), the learnable prompts and the domain classifier are jointly optimized by minimizing the total loss:
          </p>

          <p>
          \[
          \mathcal{L}_{\mathrm{total}}(\mathcal{B}) = \mathcal{L}_{\mathrm{memorize}}(\mathcal{B}) + \mathcal{L}_{\mathrm{forget}}(\mathcal{B}) + \mathcal{L}_{\mathrm{domain}}(\mathcal{B}).
          \]
          </p>
        </div>

        <h3 class="title is-4">Instance-wise Prompt Generator (InstaPG)</h3>
        <div class="content has-text-centered">
            <img src="./static/images/instaPG.png" alt="Reward-Adaptive Time-Rescaled Langevin SDE" width="40%">
        </div>
        <div class="content has-text-justified">
        <p>
          Domain is often ambiguous. The term “illustration,” for example, encompasses a broad spectrum of styles, from highly realistic renderings that closely resemble real-world images to highly stylized depictions resembling clipart, with each image varying in style.
        </p>

        <p>
          Given this nature of the domain, only a learnable prompt that is uniform across all the images cannot account for such an instance-level variation in the images, which may degrade the performance of domain unlearning.
        </p>

        <p>
          To address this problem, we introduce an Instance-wise Prompt Generator (InstaPG) to adjust the learnable vision prompts according to the input image patches.
        </p>

        <p>
          InstaPG is embedded in intermediate layers (i.e., the Transformer block) of the image encoder to generate additional instance-wise prompts to be fed to the subsequent layer via a cross-attention mechanism, where the learnable vision prompts serve as queries, while the image patch features act as keys and values. The generated prompts are conditioned on the input image patch features, allowing the model to effectively capture the property of the input image.
        </p>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>
        <h3 class="title is-4">Quantitative Results</h3>
        <div class="content has-text-centered">
            <img src="./static/images/table.png" alt="table" width="70%">
        </div>
        <div class="content has-text-justified">
          <p>
            The average performance of all possible combinations of the domains to be forgotten/retained are reported for various numbers of domains to be forgotten 
            \( |D_{\text{forget}}| \in \{1, 2, 3\} \). ImageNet has only two domains, so only the case \( |D_{\text{forget}}| = 1 \) is tested.
            Performance is evaluated using the three metrics:  
            (i) <strong>Mem</strong>: the accuracy of the classes for data from memorized domains,  
            (ii) <strong>For</strong>: the error of the classes for data from forgotten domains, and  
            (iii) <strong>H</strong>: the harmonic mean of Mem and For. Higher values mean better performance.
            The <em>Baseline</em> in the table refers to the method that trains the vision prompt using 
            \( \mathcal{L}_{\mathrm{memorize}} \) and \( \mathcal{L}_{\mathrm{forget}} \).
          </p>
        </div>
        <h3 class="title is-4">Qualitative Results</h3>
        <div class="content has-text-centered">
            <img src="./static/images/attention_map.png" alt="attention map" width="60%">
        </div>
        <div class="content has-text-justified">
          <p>
            We showcase the attention heatmaps before and after applying our method, where the domain to be forgotten is real.
          </p>

          <p>
            For the forgetting data, the Zero-shot CLIP attention concentrates on the objects. After applying our method, the attention on the objects disappears or significantly weakens for the data from the domain to be forgotten (i.e., real).
          </p>

          <p>
            For the data from the domain to be memorized (i.e., painting, clipart, and sketch), our method fully maintains or strengthens previous attention on the objects.
          </p>

          <p>
            Our method suppresses prediction sensitivity for data from domains to be forgotten while preserving or enhancing sensitivity for data from domains to be memorized, enabling the model to effectively forget unwanted domain information while maintaining high accuracy on the retained domains.
          </p>
        </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @
    </code></pre>
  </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Website adapted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and <a href="https://joonghyuk.com/instantdrag-web/">InstantDrag</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>


</body>
</html>
